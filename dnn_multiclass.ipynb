{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541af361-f362-487c-b15c-f607a982246f",
   "metadata": {},
   "source": [
    "# Multiclass classification with deep neural networks\n",
    "This is deep neural network implementation from scratch, without using any machine learning libraries.\n",
    "\n",
    "The dataset used is MNIST dataset of handwritten images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526df53-9604-48df-b38f-7e3003bc4fb6",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Load data](#1)\n",
    "- [2 - Initialize Parameters](#2)\n",
    "- [3 - Cost Function](#3)\n",
    "- [4 - Activation Function](#4)\n",
    "- [5 - Forward Propagation](#5)\n",
    "- [6 - Backward Propagation](#6)\n",
    "- [7 - Update Parameters](#7)\n",
    "- [8 - Train Model](#8)\n",
    "- [9 - Save and Load Model](#9)\n",
    "- [10 - Predict](#10)\n",
    "- [11 - Print Images](#11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d146ab8-0a15-40bb-a251-00fc072193f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from utils import * \n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43391347-9a10-4600-8408-546a02db5478",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "### Load data\n",
    "Dataset contains training and test sets.\n",
    "Each example is flattened `28x28` greyscale image of with shape `784 (28 * 28)`.\n",
    "\n",
    "By default label set is a vector that contains single digit from `0` to `9`. \n",
    "In order to make it work first we need to expand this into a 10 dimensional matrix\n",
    "e.g if the label is `4` we expand it to be `[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]`\n",
    "After this the label set shape will become `10 x 60000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2726339-25e7-46fa-bf5f-b3d28ccaf8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  60000\n",
      "Number of testing examples:   10000\n",
      "------\n",
      "Flattened image size:         784\n",
      "------\n",
      "x_train shape:                (784, 60000)\n",
      "y_train shape initial:        (1, 60000)\n",
      "------\n",
      "y_train shape expanded:       (10, 60000)\n",
      "------\n",
      "x_test shape:                 (784, 10000)\n",
      "y_test shape:                 (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train_raw, x_test, y_test = load_mnist() # Load mnist dataset\n",
    "\n",
    "m_train = x_train.shape[1] # Number of examples\n",
    "num_px = x_train.shape[0] # Number of input elements\n",
    "m_test = x_test.shape[1] # Number of test examples\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train = x_train/255.\n",
    "x_test = x_test/255.\n",
    "\n",
    "# Expand Y values to vectors\n",
    "# e.g. 4 will become [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "y_train = np.zeros((10, y_train_raw.shape[1]))\n",
    "m = list(range(y_train_raw.shape[1]))\n",
    "n = list(y_train_raw.squeeze())\n",
    "y_train[n[:], m[:]] = 1\n",
    "\n",
    "print ('Number of training examples: ', m_train)\n",
    "print ('Number of testing examples:  ', m_test)\n",
    "print ('------')\n",
    "print ('Flattened image size:        ', num_px)\n",
    "print ('------')\n",
    "print ('x_train shape:               ', x_train.shape)\n",
    "print ('y_train shape initial:       ', y_train_raw.shape)\n",
    "print ('------')\n",
    "print ('y_train shape expanded:      ', y_train.shape)\n",
    "print ('------')\n",
    "print ('x_test shape:                ', x_test.shape)\n",
    "print ('y_test shape:                ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5c767-2bda-443a-be81-1cd21a8be1ec",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b74188-3068-4d43-9edc-bd2b1264cdc5",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46bc8204-6f50-4bf4-98e8-3884b92f7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dims):\n",
    "    \"\"\"\n",
    "    Initializes parameters\n",
    "        Wl -- to random values\n",
    "        bl -- to zeros\n",
    "        \n",
    "    Arguments:\n",
    "        dims -- list of layer sizes\n",
    "    \n",
    "    Returns:\n",
    "        parameters\n",
    "            Wl -- weights matrix, shape (current layer nodes, previous layer nodes)\n",
    "            bl -- biases vector, shape (current layer nodes, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(dims) # Number of layers\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(dims[l], dims[l-1]) / np.sqrt(dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (dims[l], dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d857a2-15cf-455a-959f-cc89e734170b",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## Cost Function\n",
    "\n",
    "Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{1}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f621bdd-5b33-4f59-9d3f-2a0ca9f3f0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "\n",
    "    Arguments:\n",
    "        AL -- output vector(predictions), shape (number of classes, number of examples)\n",
    "        Y -- actual values matrix, shape (number of classes, number of examples)\n",
    "\n",
    "    Returns:\n",
    "        cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]    \n",
    "    n = Y.shape[0]\n",
    "    cost = np.zeros((n, 1)) + 1\n",
    "    \n",
    "    for i in range(n):\n",
    "        al = AL[i, :]\n",
    "        y = Y[i, :]\n",
    "        cost[i, :] = (1./m) * (-np.dot(y, np.log(al).T) - np.dot(1-y, np.log(1-al).T))\n",
    "    \n",
    "#     cost = (1./m) * (-np.dot(Y, np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T)) \n",
    "#     cost = np.squeeze(cost)    \n",
    "\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b52f2-446e-4672-9e19-5bd6e77f006e",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "### Activation Function\n",
    "\n",
    "In this notebook, you will use two activation functions:\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$.\n",
    "\n",
    "\n",
    "\n",
    "- **ReLU**: $A = RELU(Z) = max(0, Z)$. \n",
    "\n",
    "\n",
    "- **`sigmoid_backward`**: Implements the backward propagation for SIGMOID unit.\n",
    "\n",
    "- **`relu_backward`**: Implements the backward propagation for RELU unit.\n",
    "\n",
    "If $g(.)$ is the activation function, \n",
    "`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}). \\tag{2}$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4294f1e2-f9b1-4417-b68a-3b018e7ecee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Calculates sigmoid of any input\n",
    "    \n",
    "    Arguments:\n",
    "        Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "        A -- sigmoid of input Z, shape (same as Z)\n",
    "        cache -- returns Z for caching, useful in backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Calculates RELU.\n",
    "\n",
    "    Arguments:\n",
    "        Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "        A -- Activation of Z\n",
    "        cache -- returns Z for caching, useful in backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert A.shape == Z.shape, 'A should be same shape as Z'\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Calculates the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        cache -- returns Z for caching, useful in backpropagation\n",
    "\n",
    "    Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # convert dz to a numpy array.\n",
    "    \n",
    "    # When z <= 0, set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert dZ.shape == Z.shape, 'dZ should be the same shape as Z'\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Calculates the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "        dA -- post-activation gradient, of any shape\n",
    "        cache -- returns Z for caching, useful in backpropagation\n",
    "\n",
    "    Returns:\n",
    "        dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert dZ.shape == Z.shape, 'dZ should be the same shape as Z'\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b60ac5-49cc-414a-b848-3d3027f10770",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## Forward Propagation\n",
    "Calculate forward propagation:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{3}$$\n",
    "Followed by an activation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb15cb9-26dd-47a4-a3e1-28af07a88a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Calculates activations of Z and passes to the next layer.\n",
    "    \n",
    "    Arguments:\n",
    "        A_prev -- activations of previous layer\n",
    "        W -- weight matrix of the current layer\n",
    "        b -- bias values of the current layer\n",
    "        activation -- type of activation function\n",
    "    \"\"\"\n",
    "    activation = activation.lower()\n",
    "    assert activation == 'relu' or activation == 'sigmoid', \"activation should be either 'relu', or 'sigmoid'\"\n",
    "    \n",
    "    Z = W.dot(A_prev) + b\n",
    "    assert Z.shape == (W.shape[0], A_prev.shape[1]), 'wrong Z shape'\n",
    "\n",
    "    linear_cache = (A_prev, W, b)   \n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)    \n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert A.shape == (W.shape[0], A_prev.shape[1]), 'wrong A shape'\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e3dbee-2709-4f24-a4be-4ed7c77b122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X, parameters):\n",
    "    \"\"\"\n",
    "    Propagates forward.\n",
    "    \n",
    "    Arguments:\n",
    "        X -- numpy array of input data, shape (number of examples, input size)\n",
    "        parameters -- initialized parameters\n",
    "    \n",
    "    Returns:\n",
    "        AL -- output layer of predictions\n",
    "        caches -- list of caches containing:\n",
    "                    cache of relu activations\n",
    "                    cache of sigmoid activation of output layer\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # number of layers\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = forward_pass(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)        \n",
    "    \n",
    "    AL, cache = forward_pass(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert AL.shape == (dims[len(dims) - 1],X.shape[1]), 'wrong output layer shape'\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dc3620-4af5-4fbb-895e-db28de3c9bcb",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "### Backward Propagation\n",
    "\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
    "\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{4}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{5}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f318aa-1360-4e12-9ceb-1b707e2c5ee9",
   "metadata": {},
   "source": [
    "Pass parameters back by one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f45009-52b6-48ee-b88b-c4fc5dd305f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Calculates derivative of activations and passes to the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "        dA -- post-activation gradient of the current layer\n",
    "        cache -- tuple of caches (linear_cache, activation_cache), saved during forward propagation\n",
    "        activation -- name of activation function\n",
    "    \n",
    "    Returns:\n",
    "        dA_prev -- Gradient of the cost with respect to the activation of the previous layer, same shape as A_prev\n",
    "        dW -- Gradient of the cost with respect to W, same shape as W\n",
    "        db -- Gradient of the cost with respect to b, same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "        \n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]    \n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ffb706-b532-4849-86a0-c1b6923760bf",
   "metadata": {},
   "source": [
    "Backward propagate through all the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3238b4aa-b699-4cfd-82bc-74210a994910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagate(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Propagates backwards\n",
    "    \n",
    "    Arguments:\n",
    "        AL -- output layer predictions\n",
    "        Y -- actual values\n",
    "        caches -- list of caches containing:\n",
    "                    cache of relu activations\n",
    "                    cache of sigmoid activations of the last layer\n",
    "    \n",
    "    Returns:\n",
    "        grads -- A dictionary of gradients 'dA', 'dW', 'db'\n",
    "\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initialize the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = backward_pass(dAL, current_cache, activation = \"sigmoid\")\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = backward_pass(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80844b5-76e7-4230-bc42-487075a08931",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "### Update Parameters\n",
    "\n",
    "Update the parameters of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{8}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{9}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. \n",
    "\n",
    "After computing the updated parameters, store them in the parameters dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad31731-6938-42ba-afd5-527e96746b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters.\n",
    "    \n",
    "    Arguments:\n",
    "        parameters -- dictionary of parameters \n",
    "        grads -- dictionary of gradients\n",
    "    \n",
    "    Returns:\n",
    "        parameters -- dictionary of updated parameters 'W' and 'b'\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc77d179-1bdd-40f7-bb86-6ac02933247f",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "### Train Model\n",
    "\n",
    "User above defined functions to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70c17eb1-949f-4ad3-879c-c35f96115a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, dims, learning_rate = 0.01, epochs = 100, print_cost=False):\n",
    "    \"\"\"\n",
    "    Trains full deep neural network.\n",
    "    \n",
    "    Arguments:\n",
    "        X -- numpy array of input data, shape (flattened image pixel number, number of examples)\n",
    "        Y -- actual values\n",
    "        layers_dims -- list of layers\n",
    "        learning_rate -- learning rate\n",
    "        epochs -- number of iterations\n",
    "        print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "        parameters -- learned parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []\n",
    "\n",
    "    parameters = initialize_parameters(dims)\n",
    "        \n",
    "    # Gradient descent\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        AL, caches = forward_propagate(X, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        grads = backward_propagate(AL, Y, caches)\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == epochs - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == epochs:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57db1ccf-9a17-445e-96a4-d8394996a7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: [0.70142882 0.76044444 0.63946462 0.69396326 0.6670153  0.6770334\n",
      " 0.67782827 0.64068001 0.67950968 0.79784079]\n",
      "Cost after iteration 100: [0.31605666 0.29147598 0.33551197 0.342628   0.30701763 0.29902622\n",
      " 0.32461938 0.31418076 0.36498056 0.33963148]\n",
      "Cost after iteration 200: [0.25742714 0.27483235 0.31060692 0.32141799 0.27738903 0.28791111\n",
      " 0.30715214 0.2818912  0.35436543 0.32388542]\n",
      "Cost after iteration 300: [0.18707768 0.23999375 0.27926415 0.28888798 0.23764823 0.27377348\n",
      " 0.28176694 0.23170926 0.34128323 0.29773221]\n",
      "Cost after iteration 400: [0.13579453 0.18893308 0.24234842 0.24856974 0.19640012 0.25422108\n",
      " 0.24433614 0.17882303 0.32519127 0.26549118]\n",
      "Cost after iteration 500: [0.10728432 0.1413124  0.20704532 0.21105611 0.16624901 0.23202257\n",
      " 0.196418   0.14199014 0.30702334 0.24178974]\n",
      "Cost after iteration 600: [0.09003888 0.10896511 0.17799246 0.18140251 0.14624879 0.21182066\n",
      " 0.15102677 0.11916893 0.28534504 0.22686139]\n",
      "Cost after iteration 700: [0.07856165 0.08918582 0.15591175 0.16009061 0.13251355 0.19471895\n",
      " 0.11955836 0.10447218 0.26103189 0.21571206]\n",
      "Cost after iteration 800: [0.07060603 0.07663473 0.13961813 0.14497506 0.12260606 0.18032763\n",
      " 0.09995417 0.09441721 0.23773057 0.20608105]\n",
      "Cost after iteration 900: [0.06479053 0.06809739 0.12745379 0.13378063 0.11493438 0.16820003\n",
      " 0.08721687 0.08722139 0.21705561 0.19719037]\n",
      "Cost after iteration 1000: [0.06031387 0.06196361 0.11820909 0.12511494 0.10867599 0.15787941\n",
      " 0.07834773 0.0819113  0.19950225 0.1885696 ]\n",
      "Cost after iteration 1100: [0.05671315 0.05736603 0.11105694 0.11826052 0.10335873 0.14906362\n",
      " 0.07179444 0.07790408 0.18489747 0.18001033]\n",
      "Cost after iteration 1200: [0.05372419 0.05379284 0.10541506 0.11274898 0.09870138 0.14148692\n",
      " 0.06674202 0.07480662 0.17271809 0.17149153]\n",
      "Cost after iteration 1300: [0.05118006 0.05094539 0.1008618  0.10826346 0.09451934 0.13496093\n",
      " 0.06272113 0.07235869 0.16248262 0.16311234]\n",
      "Cost after iteration 1400: [0.04897446 0.04862214 0.0970983  0.10456867 0.09071984 0.12930381\n",
      " 0.05944462 0.0703644  0.15376558 0.15500614]\n",
      "Cost after iteration 1500: [0.04703881 0.04668639 0.09392001 0.10148165 0.08726647 0.12434915\n",
      " 0.05672377 0.06868869 0.14623417 0.14729066]\n",
      "Cost after iteration 1600: [0.04531827 0.04504575 0.0911955  0.09885755 0.0841387  0.11996862\n",
      " 0.05442627 0.06723325 0.13964485 0.14005755]\n",
      "Cost after iteration 1700: [0.04377035 0.04363825 0.08882813 0.09661361 0.08132048 0.11606444\n",
      " 0.05245608 0.06594653 0.1337805  0.13338531]\n",
      "Cost after iteration 1800: [0.04237107 0.04241508 0.08674896 0.09464465 0.07878879 0.11260147\n",
      " 0.05074541 0.06478747 0.12853149 0.12731682]\n",
      "Cost after iteration 1900: [0.04110223 0.0413475  0.08490342 0.09287423 0.07653091 0.1095216\n",
      " 0.04924457 0.06371958 0.12382977 0.12185053]\n",
      "Cost after iteration 2000: [0.03995013 0.04041034 0.08325418 0.09123693 0.07451382 0.1067651\n",
      " 0.04791271 0.062729   0.11959486 0.11698502]\n",
      "Cost after iteration 2100: [0.03889967 0.03958187 0.08175988 0.0897184  0.0727204  0.10427115\n",
      " 0.04671766 0.06180117 0.11576254 0.11267586]\n",
      "Cost after iteration 2200: [0.03794159 0.03884734 0.08039336 0.08830272 0.07112226 0.10199246\n",
      " 0.04563555 0.0609285  0.1122724  0.10886682]\n",
      "Cost after iteration 2300: [0.03706348 0.03819007 0.0791311  0.0869849  0.06968959 0.09990584\n",
      " 0.04464766 0.06010447 0.10906252 0.10549441]\n",
      "Cost after iteration 2400: [0.03625245 0.03759836 0.07795909 0.08574611 0.06839917 0.09798088\n",
      " 0.04374262 0.05932179 0.10609271 0.1024971 ]\n",
      "Cost after iteration 2499: [0.03550896 0.03706696 0.07687622 0.08459101 0.06724221 0.09620137\n",
      " 0.04291536 0.05859073 0.10335595 0.09984333]\n"
     ]
    }
   ],
   "source": [
    "input_layer = x_train.shape[0] # set input layer size\n",
    "dims = [input_layer, 128, 128, 10] # set layer number and sizes\n",
    "\n",
    "# parameters, costs = model(x_train[:, :10000], y_train[:, :10000], dims, learning_rate=0.0075, epochs = 100, print_cost = True)\n",
    "parameters, costs = model(x_train, y_train, dims, learning_rate=0.0075, epochs = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde9c42-0efe-48b6-a8b0-05aebdd098ee",
   "metadata": {},
   "source": [
    "<a name='9'></a>\n",
    "## Save and Load Model\n",
    "\n",
    "Save model to csv files for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a82ae143-c7f4-4aaa-a8d1-06fb8b725892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(parameters):\n",
    "    path = './model/'\n",
    "    f = open(path + 'param_keys.csv', 'w')\n",
    "    w = csv.writer(f, delimiter=',')\n",
    "    keys = list(parameters.keys())\n",
    "    w.writerow(keys)\n",
    "    f.close()\n",
    "    \n",
    "    for key in keys:        \n",
    "        np.savetxt(path + key + '.csv', parameters[key], delimiter=\",\")        \n",
    "        \n",
    "def load_model():\n",
    "    path = './model/'\n",
    "    parameters = {}\n",
    "    param_keys = []\n",
    "    with open(path + 'param_keys.csv', newline='') as csvfile:\n",
    "        content = csv.reader(csvfile, delimiter=',', quotechar='|')            \n",
    "        for row in content:\n",
    "            if len(row):\n",
    "                param_keys = list(row)\n",
    "    for key in param_keys:\n",
    "        p = np.genfromtxt(path + key + '.csv', delimiter=',')\n",
    "        parameters[key] = p.reshape(p.shape[0], -1)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8967eee1-4c19-4825-8578-5ef35eae3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(parameters)\n",
    "parameters = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84792498-4dd7-410e-9ba0-9cd11e180d69",
   "metadata": {},
   "source": [
    "<a name='10'></a>\n",
    "## Predict\n",
    "\n",
    "Use forward propagation to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbea25fd-a3a8-4c5d-83e1-a51e359dc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    Predicts using learned parameters.\n",
    "    \n",
    "    Arguments:\n",
    "        X -- input data\n",
    "        parameters -- parameters of the prevously trained model\n",
    "        \n",
    "    Returns:\n",
    "        right_preds -- dictionary of the right predictions\n",
    "        wrong_preds -- dictionary of the wrong predictions\n",
    "        low_confidence_preds -- dictionary of the predictions with low confidence\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1] # number of examples\n",
    "    sum_rights = 0 # number of correct predictions\n",
    "    \n",
    "    predictions = {\n",
    "        \"right\": {},\n",
    "        \"wrong\": {},\n",
    "        \"low_confidence\": {}\n",
    "    }\n",
    "\n",
    "    # Forward propagation\n",
    "    preds, caches = forward_propagate(X, parameters)\n",
    "\n",
    "    for i in range(preds.shape[1]):\n",
    "        max_pred = preds[:, i]\n",
    "        prediction = np.argmax(max_pred, axis=None)\n",
    "        actual = y[:, i].squeeze()\n",
    "        \n",
    "        if max(max_pred) < 0.1: \n",
    "            predictions['low_confidence'][i] = {\n",
    "                'pred': prediction,\n",
    "                'actual': int(actual),\n",
    "                'confidence': preds[prediction, i]\n",
    "            }\n",
    "        elif prediction == actual:\n",
    "            sum_rights += 1\n",
    "            predictions['right'][i] = {\n",
    "                'pred': prediction,\n",
    "                'actual': int(actual),\n",
    "                'confidence': preds[prediction, i]\n",
    "            }\n",
    "        else:\n",
    "            predictions['wrong'][i] = {\n",
    "                'pred': prediction,\n",
    "                'actual': int(actual),\n",
    "                'confidence': preds[prediction, i]\n",
    "            }\n",
    "\n",
    "    accuracy = sum_rights/m\n",
    "    \n",
    "    return accuracy, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb6d03fe-257e-4833-8baf-734df978a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:        0.9069\n",
      "correct:         9069\n",
      "mistakes:        883\n",
      "low confidence:  48\n"
     ]
    }
   ],
   "source": [
    "accuracy, predictions = predict(x_test, y_test, parameters)\n",
    "print('accuracy:       ', accuracy)\n",
    "print('correct:        ', len(predictions['right']))\n",
    "print('mistakes:       ', len(predictions['wrong']))\n",
    "print('low confidence: ', len(predictions['low_confidence']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d440018-12d8-42a1-8b3d-256254f9345b",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a9f30-6293-4a87-9ff6-86291cce5003",
   "metadata": {},
   "source": [
    "#### Print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaecf184-8d0e-465c-8bcc-c574ad8273f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 {'pred': 2, 'actual': 6, 'confidence': 0.09720772600947349}\n",
      "97 {'pred': 7, 'actual': 7, 'confidence': 0.05722972289868852}\n",
      "495 {'pred': 0, 'actual': 8, 'confidence': 0.05098865102802033}\n",
      "514 {'pred': 6, 'actual': 6, 'confidence': 0.0223158585499284}\n",
      "552 {'pred': 0, 'actual': 0, 'confidence': 0.07597712125726902}\n",
      "659 {'pred': 9, 'actual': 2, 'confidence': 0.07938259569166886}\n",
      "992 {'pred': 9, 'actual': 9, 'confidence': 0.08367031377019608}\n",
      "998 {'pred': 8, 'actual': 8, 'confidence': 0.046734030735870714}\n",
      "1101 {'pred': 2, 'actual': 8, 'confidence': 0.08134865051168073}\n",
      "1128 {'pred': 3, 'actual': 3, 'confidence': 0.03096905313235712}\n",
      "1444 {'pred': 0, 'actual': 6, 'confidence': 0.07549845788451195}\n",
      "1678 {'pred': 0, 'actual': 2, 'confidence': 0.030116186142989592}\n"
     ]
    }
   ],
   "source": [
    "print_num = 10\n",
    "num = 0\n",
    "\n",
    "for i in predictions['low_confidence']:\n",
    "    print(i, predictions['low_confidence'][i])\n",
    "    if num > print_num:\n",
    "        break\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50588ac4-804d-4260-b8d0-49a525fed23c",
   "metadata": {},
   "source": [
    "<a name='11'></a>\n",
    "### Print Images\n",
    "print images in predictions dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8419794d-eb69-45ba-b276-858b387f9a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_images(X, p, num_images=10):\n",
    "    \"\"\"\n",
    "    Plots images where predictions and truth were different.\n",
    "        X -- dataset\n",
    "        y -- true labels\n",
    "        p -- predictions\n",
    "    \"\"\"\n",
    "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
    "    plt_num = 1\n",
    "    row = 1\n",
    "    for index in p:\n",
    "        plt.subplot(1, num_images, plt_num)\n",
    "        plt.imshow(X[:, index].reshape(28,28), interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Prediction: \" + str(p[index]['pred']) + \" \\n Actual: \" + str(p[index]['actual']))\n",
    "        \n",
    "        plt_num += 1\n",
    "        if plt_num % 10 == 0: \n",
    "            row+= 1\n",
    "        if plt_num >= num_images:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b88ca22-73c8-46cf-8495-6173e8157c24",
   "metadata": {},
   "source": [
    "Print wrongly predicted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1337e0e-2b5c-4de4-9f23-2c9f41d9b9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x2880 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_images(x_test, predictions['wrong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732bf1a7-76cb-4a89-85dc-6da6f16cd5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
